{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2700551c-e115-4ae1-b6c4-9ebac0eb3c6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Health Updates Challenge\n",
    "This notebook is used only as part 1 of the challenge and as a setup for the workflow that is run by notebooks 1 and 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "398bae49-cd58-4f6d-ae27-a2fdafa0607e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Mounting the ADLS container\n",
    "The health-updates container was previously created through azure portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14b81968-cf72-4e9f-9a4d-b147cbb5ff29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbutils.fs.unmount('/mnt/health-updates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31b398fa-20da-4363-85aa-840ad464fcd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#databricks url = #secrets/createScope\n",
    "application_id = dbutils.secrets.get(scope=\"databricks-secrets-nsp\", key=\"application-id\") \n",
    "directory_id = dbutils.secrets.get(scope=\"databricks-secrets-nsp\", key=\"directory-id\")\n",
    "secret = dbutils.secrets.get(scope=\"databricks-secrets-nsp\", key=\"secretv2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32e619fa-b38d-4795-a7a1-99a895b21e10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "container_name = 'health-updates'\n",
    "account_name = 'datalakensp'\n",
    "mount_point = '/mnt/health-updates'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6edc4fb8-6d90-428d-bd2c-5c741707cbcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "          \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "          \"fs.azure.account.oauth2.client.id\": application_id,\n",
    "          \"fs.azure.account.oauth2.client.secret\": secret,\n",
    "          \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{directory_id}/oauth2/token\"}\n",
    "dbutils.fs.mount(source = f\"abfss://{container_name}@{account_name}.dfs.core.windows.net/\",\n",
    "mount_point = mount_point,\n",
    "extra_configs = configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19295427-8e99-4498-9cc0-2c3d54ca1784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Process the data\n",
    "Process the 'health_status_updates.csv' file from the bronze folder into the silver folder and call it 'health_date' with an additional column called 'updated_timestamp' consisting of the current_timestamp at which the data is inserted into the silver folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee3f9daa-9078-4d37-813a-20e87982696a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, DoubleType, StructField, StructType\n",
    "\n",
    "mount_point = '/mnt/health-updates'\n",
    "health_data_path = f\"{mount_point}/bronze/health_status_updates.csv\"\n",
    "\n",
    "health_data_schema = StructType([\n",
    "                    StructField(\"STATUS_UPDATE_ID\", IntegerType(), False),\n",
    "                    StructField(\"PATIENT_ID\", IntegerType(), False),\n",
    "                    StructField(\"DATE_PROVIDED\", StringType(), False),\n",
    "                    StructField(\"FEELING_TODAY\", StringType(), True),\n",
    "                    StructField(\"IMPACT\", StringType(), True),\n",
    "                    StructField(\"INJECTION_SITE_SYMPTOMS\", StringType(), True),\n",
    "                    StructField(\"HIGHEST_TEMP\", DoubleType(), True),\n",
    "                    StructField(\"FEVERISH_TODAY\", StringType(), True),\n",
    "                    StructField(\"GENERAL_SYMPTOMS\", StringType(), True),\n",
    "                    StructField(\"HEALTHCARE_VISIT\", StringType(), True)\n",
    "                    ]\n",
    "                    )\n",
    "\n",
    "health_updates = spark.read.csv(path = health_data_path, header=True, schema=health_data_schema)\n",
    "health_updates.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f297cf11-7ae8-429e-b528-b7015d53b064",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, current_timestamp\n",
    "health_updates = health_updates.select(\n",
    "                                'STATUS_UPDATE_ID',\n",
    "                                'PATIENT_ID',\n",
    "                                to_date(health_updates['DATE_PROVIDED'],'MM/dd/yyyy').alias('DATE_PROVIDED'),\n",
    "                                'FEELING_TODAY',\n",
    "                                'IMPACT',\n",
    "                                'INJECTION_SITE_SYMPTOMS',\n",
    "                                'HIGHEST_TEMP',\n",
    "                                'FEVERISH_TODAY',\n",
    "                                'GENERAL_SYMPTOMS',\n",
    "                                'HEALTHCARE_VISIT',\n",
    "                                current_timestamp().alias(\"UPDATED_TIMESTAMP\")\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99e616eb-0bc1-4143-aa4c-8961e312dc4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Creating the database and storing the data\n",
    "'health_data' should be external delta lake format with the underlying data in the silver folder and the table itself as part of a new 'healthcare' database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c61697e-2ad1-49f2-9fb7-5ac9cf1c3c24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE DATABASE IF NOT EXISTS healthcare;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16123f5e-5968-49a6-965b-b96d7af46a18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "health_updates.write.format(\"delta\").mode('overwrite').option(\"path\", f\"{mount_point}/silver/health_data\").saveAsTable(\"healthcare.health_data\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5483565796418825,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "0_setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
